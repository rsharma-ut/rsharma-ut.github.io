


<p>library(tidyverse) library (carData) library(fivethirtyeight) library(ggplot2)</p>
<div id="my-first-dataset-i-chose-was-the-amount-of-murder-rape-and-assault-arrests-made-in-each-state-along-with-the-percent-urban-population-each-states-population-makes-up." class="section level1">
<h1>My first dataset I chose was the amount of Murder, Rape, and Assault Arrests made in each state along with the percent urban population each state's population makes up.</h1>
</div>
<div id="my-second-dataset-provides-data-regarding-fatal-collisions-in-each-state-and-the-percent-of-those-drivers-that-were-alcohol-impaired-speeding-not-distracted-not-involved-in-previous-accidents-and-number-of-drivers-involved-in-fatal-collisions.-it-also-provided-data-regarding-car-insurance-premiums-and-losses-those-companies-incurred-due-to-accidents.-all-of-this-data-was-found-across-each-state." class="section level1">
<h1>My second dataset provides data regarding fatal collisions in each state and the percent of those drivers that were alcohol-impaired, speeding, not distracted, not involved in previous accidents, and number of drivers involved in fatal collisions. It also provided data regarding car insurance premiums and losses those companies incurred due to accidents. All of this data was found across each state.</h1>
</div>
<div id="the-data-regarding-fatal-collisions-was-gathered-from-historic-data-and-insurance-company-information-while-the-arrest-data-was-gathered-form-the-mcneil-monograph-and-from-the-statistical-abstracts-from-1975.-since-this-data-is-from-1975-it-might-not-be-reflective-of-modern-society-statistics-as-many-societal-changes-have-incurred-over-time." class="section level1">
<h1>The data regarding fatal collisions was gathered from historic data and insurance company information, while the arrest data was gathered form the McNeil monograph and from the Statistical Abstracts from 1975. Since this data is from 1975, it might not be reflective of modern society statistics as many societal changes have incurred over time.</h1>
</div>
<div id="i-liked-these-datasets-because-it-is-interesting-to-see-the-various-statistics-they-have-about-arrests-in-each-state-and-how-they-relate-to-each-other.-does-one-state-have-on-average-more-rape-arrests-only-or-do-they-have-more-in-all-three-categories" class="section level1">
<h1>I liked these datasets because it is interesting to see the various statistics they have about Arrests in each state and how they relate to each other. Does one state have on average more rape arrests only or do they have more in all three categories?</h1>
</div>
<div id="i-also-like-the-bad_drivers-dataset-because-you-only-really-hear-about-drunk-driving-on-the-news-so-that-is-how-you-think-about-most-fatal-collisions.-it-is-interesting-to-explore-the-different-reasons-behind-fatal-collisions-to-see-how-they-compare-against-each-other." class="section level1">
<h1>I also like the bad_drivers dataset because you only really hear about drunk driving on the news so that is how you think about most fatal collisions. It is interesting to explore the different reasons behind fatal collisions to see how they compare against each other.</h1>
</div>
<div id="since-my-dataset-was-already-tidy-i-made-it-untidy-with-pivot_wider-by-pulling-the-values-from-num_driver-to-make-those-the-new-columns-and-put-their-corresponding-values-from-perc_speeding-in-their-columns" class="section level1">
<h1>since my dataset was already tidy, I made it untidy with pivot_wider by pulling the values from 'num_driver' to make those the new columns, and put their corresponding values from 'perc_speeding' in their columns</h1>
<p>bad_drivers &lt;- bad_drivers BDW &lt;- bad_drivers %&gt;% pivot_wider(names_from=&quot;num_drivers&quot;, values_from=&quot;perc_speeding&quot;) head(BDW)</p>
</div>
<div id="to-re-tidy-my-data-i-used-pivot_longer-to-retake-those-columns" class="section level1">
<h1>to re-tidy my data, I used pivot_longer to retake those columns</h1>
</div>
<div id="and-put-them-into-1-column-with-its-values-representing-the-percent-of-drivers" class="section level1">
<h1>and put them into 1 column with its values representing the percent of drivers</h1>
</div>
<div id="involved-in-fatal-accidents-who-were-speeding." class="section level1">
<h1>involved in fatal accidents who were speeding.</h1>
</div>
<div id="i-then-removed-the-nas-from-that-column-to-condense-the-dataset" class="section level1">
<h1>I then removed the NAs from that column to condense the dataset</h1>
</div>
<div id="so-that-there-werent-rows-of-the-same-state-with-empty-values" class="section level1">
<h1>so that there weren't rows of the same state with empty values</h1>
<p>baddrive_fixed &lt;- BDW %&gt;% pivot_longer(7:51, names_to=&quot;amt_drivers&quot;, values_to=&quot;percent_speeding&quot;) baddrive_fixed &lt;- baddrive_fixed %&gt;%filter(!is.na(percent_speeding)) head(baddrive_fixed)</p>
</div>
<div id="now-i-converted-my-amt_drivers-column-from-a-character-column-into-a-numeric-column" class="section level1">
<h1>now I converted my amt_drivers column from a character column, into a numeric column</h1>
<p>baddrive_fixed<span class="math inline">\(amt_drivers &lt;- as.numeric(as.character(baddrive_fixed\)</span>amt_drivers))</p>
</div>
<div id="here-the-original-data-set-didnt-have-a-state-column-but-rather-had-the-row-names" class="section level1">
<h1>here, the original data set didn't have a state column, but rather had the row names</h1>
</div>
<div id="as-states-so-here-i-converted-the-state-row-names-into-a-column-entitled-state" class="section level1">
<h1>as states, so here I converted the state row names into a column entitled 'state'</h1>
<p>USArrests &lt;- USArrests d &lt;- USArrests %&gt;% rownames_to_column(&quot;state&quot;)</p>
</div>
<div id="now-i-untidyed-the-dataset-with-pivot_wider-and-then-retidyed-the-dataset-using-pivot_longer-and-removed-all-the-duplicate-rows-w-nas" class="section level1">
<h1>now, I untidyed the dataset with pivot_wider and then retidyed the dataset using pivot_longer, and removed all the duplicate rows w NAs</h1>
<p>US_wide &lt;- d %&gt;% pivot_wider(names_from=&quot;state&quot;, values_from=&quot;Rape&quot;) dim(US_wide) USA_fixed &lt;- US_wide %&gt;% pivot_longer(4:53, names_to=&quot;state&quot;, values_to=&quot;Rape&quot;) %&gt;% filter(!is.na(Rape)) USA_fixed &lt;- USA_fixed %&gt;% select(state,everything()) #at the end of my tidying, I just moved the state column from the end to the first column in the dataset</p>
</div>
<div id="i-did-an-inner-join-joining-both-of-my-datasets-on-the-common-id-variable-state" class="section level1">
<h1>I did an inner join, joining both of my datasets on the common ID variable 'state'</h1>
<p>full &lt;- inner_join(USA_fixed,baddrive_fixed) #the USA_fixed dataset has 50 observations while the baddrive_fixed dataset has 51 observations, because it included District of Colombia. #1 observation was dropped since I did an inner join and the District of Colombia did not have corresponding data in the USA_fixed dataset #I chose to do an inner join because the state column matched pretty well in both datasets, other than the fact that baddrive_fixed contained an observation for District of Colombia and USA_fixed did not #inner join then dropped that observation since it did not have corresponding values in the USA_fixed dataset, so I just dropped that observation as a whole so we could just look at the data for the 50 states #since the only observation that was dropped was that of the District of Colombia, I don't foresee this skewing my data because we still have a relatively large dataset</p>
<p>full %&gt;% select(state, Rape) %&gt;% group_by(state) %&gt;% filter(Rape&gt;30) %&gt;% arrange(-Rape) #here I used the select function to only look at the state and the Rape columns so that we can focus on the number of arrests that were due to rape for each state #I then grouped the dataset by state so that if there were multiple observations concerning the same state they would be grouped together #then, I used the filter function to only look at the those states with number of rape arrests that were greater than 30 per 100,000 people #then I arranged the rape column in ascending order, from those states with the highest number of rape arrests to those with the lowest number of rape arrests</p>
<p>full_mutate &lt;- full %&gt;% mutate(net_ins_profit = insurance_premiums-losses) #here, I added a new column indicating the net profit insurance companies made per state from their premiums minus the amount they lost due to car accidents</p>
<p>full_mutate %&gt;% summarize_at(c(&quot;Rape&quot;,&quot;insurance_premiums&quot;,&quot;percent_speeding&quot;), mean, na.rm=T) #here, I looked at the columns indicating the number of arrests that were rape arrests, the insurance premiums, and the percent of drivers involved in fatal accidents that were speeding,and I got the average values across all 50 states for these 3 categories</p>
<p>murderstat &lt;- cut((full_mutate$Murder), breaks = c(0, 10, 20), labels = c(&quot;low&quot;, &quot;high&quot;)) #here, I created a new column indicating whether each state's murder arrest value was high or low, defining high as any value above 10</p>
<p>full_mutate &lt;- full_mutate %&gt;% mutate(murderstat = murderstat) full_mutate &lt;- full_mutate %&gt;% mutate (pop_size = ifelse(UrbanPop&lt;50, &quot;small&quot;, &quot;large&quot;)) #here, I added the high/low indication of murder arrest statistics to my dataset and added another categorical column indicating whether the percent of the population that was urban was a small percent or a large percent--larger percents indicating larger urban populations</p>
<p>full_mutate %&gt;% group_by(murderstat) %&gt;% summarize(max(Rape), min(Rape)) #here I grouped my data to look at the groups with high murder stats and low murder stats, and looked at the max and minimum amount of Rape arrests made in each group</p>
<p>full_mutate %&gt;% select(Murder:net_ins_profit) %&gt;% summarize_all(sd) #here I calculated the standard deviations for each of my numeric columns</p>
<p>full_mutate %&gt;% group_by(pop_size) %&gt;% summarize(number_of_states = n(),not_distract_var = var(perc_not_distracted)) #here, I looked at the two groups of urban population size and calculated the number of states within each group and the variance within each group of the percentage of inidividuals involved in fatal collisions who were not distracted #We find that amongst the 42 states with large urban populations, on average, the variance is 117 for those with large urban populations and 926 for those with small urban populations #Since the small urban population subset only consists of 8 states, this is a rather small sample size allowing for the possibility of extreme values to skew the data which could be why the variance value is much larger than that of the large dataset</p>
<p>full_mutate %&gt;% group_by(pop_size, murderstat) %&gt;% summarize(mean(amt_drivers)) #here we see that of those states with a small percentage of urban populations, the average amount of drivers involved in fatal collisions per billion miles are approximately the same across their murder statistics, while of those states with larger urban population percents, there is a small difference, with more drivers involved in fatal collisions in those states with higher murder arrest statistics. #Once again, we need to consider our sample size in our small urban population group which could account for the lack of strong distinction.</p>
<p>library(kableExtra) options(knitr.table.format = &quot;html&quot;)</p>
<p>kable_data &lt;- full_mutate %&gt;% group_by(pop_size) %&gt;% summarize(number_of_states = n(), losses_var = var(losses), max(Murder), min(Murder)) kable_data_states &lt;- full_mutate %&gt;% group_by(murderstat) %&gt;% summarize(max(Rape), max(Assault), max(UrbanPop))</p>
<p>kable_data %&gt;% kbl() %&gt;% kable_styling() kable_data_states %&gt;% kbl() %&gt;% kable_styling() #Here I have combined some of my summary statistics into a clean table to see #The first table the summary stats for the subgroup of large vs. small urban population size #it shows the number of states in each, along with variance among the losses incurred by car insurance companies due to car accidents, and the maximum and minimum murder arrest for the subgroups #from this table we can that a majority of the states have a large percent urban population and that compared to the states with smaller urban population percents, the murder arrest values are fairly similar, but a large difference in variation across insurance company losses, probably due to the difference in subset group size.<br />
#The second table shows summary stats for the subgroup of high vs. low percentage of murder arrest stats #it shows that of those states with high murder arrests, the max amount of Rape arrests was about 44, the max amount of assault arrests was 294, and the max percent of the population that was urban was 94%. #This table shows that those states with high murder arrests also higher Rape and Assault arrests, but lower percent urban population.</p>
<p>full_mutate%&gt;%select_if(is.numeric)%&gt;%cor%&gt;%as.data.frame%&gt;% rownames_to_column%&gt;%pivot_longer(-1)%&gt;% ggplot(aes(rowname,name,fill=value))+geom_tile()+ geom_text(size=2.5, aes(label=round(value,2)))+ xlab(&quot;&quot;)+ylab(&quot;&quot;)+coord_fixed()+ scale_fill_gradient2(low=&quot;white&quot;,mid=&quot;pink&quot;,high=&quot;red&quot;)+ theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5)) #correlation matrix</p>
<p>ggplot(full_mutate, aes(x=UrbanPop, y=Rape, fill= pop_size))+ geom_point(aes(color=pop_size, size=pop_size))+ scale_size_discrete(&quot;pop_size&quot;,range=c(4,2))+ labs(x= &quot;Percent Urban Population&quot;, y= &quot;Number of Rape Arrests (per 100,000 people)&quot;, title=&quot;Rape Arrests vs. Percent Urban Population&quot;)+ scale_x_continuous(labels = function(x) paste0(x * 1, '%'))+ scale_x_continuous(breaks = seq(0, 100, 10), limits= c(0,100)) #In this plot we see the relationship between urban populations and the number of rape arrests per 100,000 people. As the percentage of a population's urban population goes up, we see an increase in the number of rape arrests. #We see a steep rather than gradual increase in rape arrests as we increase in the percent of a population's urban sector. This could lead us to hypothesize a relationship between urban population and rape arrests. The points in this plot are colored based off of large or small urban population percentage to show the relative sample of size of each, and how each contributes to the correlation.</p>
<p>ggplot(full_mutate, aes(state))+ geom_bar(aes(y=net_ins_profit, fill=murderstat), colour=&quot;black&quot;, stat=&quot;summary&quot;, fun=mean)+ theme(axis.text.y = element_text(angle=0, hjust=1, size=5))+ coord_flip()+ scale_fill_brewer(palette = &quot;Purples&quot;)+ labs(y= &quot;Net Insurance Profit&quot;, x= &quot;State&quot;, title= &quot;State vs. Net Insurance Profit&quot;)+ labs(fill= &quot;Murder Arrests Stats&quot;) #In this plot we are looking at the net profit insurance companies made per state after deducting their losses due to auto collisions from their car insurance premiums. The graph is also colored based on whether or not certain states had higher or lower murder arrests. The graph shows that there isn't really a correlatin on whether mostly high/low murder arrests had higher or lower net profits. We can tell that New Jersay is a state with low murder arrests and their car insurance companies have a higher net insurance profit. #Ohio is also a state with low murder arrests, but their insurance companies have lower net profits. Also, by looking at the coloring, we can see there is a descrepency in the amount of bars that are colored purple vs. white, indicating and unequal sample for each group. For further research, I would suggest splitting the data into half so that you are looking at groups of 25 and 25 to eliminate the confounding variable of unequal comparison groups.</p>
</div>
<div id="clustering" class="section level1">
<h1>clustering</h1>
<p>clust_dat&lt;-full_mutate%&gt;% select(perc_alcohol,perc_no_previous,percent_speeding)</p>
<p>library(cluster) sil_width&lt;-vector() #empty vector to hold mean sil width for(i in 2:10){<br />
kms &lt;- kmeans(clust_dat,centers=i) sil &lt;- silhouette(kms$cluster,dist(clust_dat)) sil_width[i]&lt;-mean(sil[,3]) } ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name=&quot;k&quot;,breaks=1:10) #On the basis of just these three variables, the graph of average silhouette widths shows that 2 clusters is the best (the highest peak!).</p>
<p>kmeans1 &lt;- clust_dat %&gt;% kmeans(2) kmeans1</p>
<p>kmeansclust &lt;- clust_dat %&gt;% mutate(cluster=as.factor(kmeans1$cluster)) kmeansclust %&gt;% ggplot(aes(x= perc_alcohol, y= percent_speeding,color=cluster)) + geom_point() #This graph of our clusters shows that the clusters are relatively distinct, but close togeter so the datapoints within the clusters are relatively close together. #But within each cluster, we see that the clusters are very spread apart. This shows there is a lot of spread within our clusters, and that there is probably greater distance between the average datapoint and other points within each cluster.</p>
<p>library(GGally) ggpairs(kmeansclust, columns=1:4, aes(color=cluster)) #This graph shows how each variable correlates with each other. We can see all the correlation values around 0.2 or 0 yielding a pretty weak correlation for any two variables. #When looking at percent of drivers who got into fatal collisions who had not had previous accidents and those who were alcohol-impaired, we see a negative correlation, meaning that as the percent of drivers involved in fatal accidents that were alcohol-impaired increase, the percent of drivers involved in fatal collisions but not previously involved in any previous accidents, decreases. #Although we see this negative correlation, the correlation is relatively weak.</p>
<p>library(cluster) pam1 &lt;- clust_dat %&gt;% pam(k=2) pam1</p>
<p>pamclust&lt;-clust_dat %&gt;% mutate(cluster=as.factor(pam1$clustering)) pamclust %&gt;% ggplot(aes(perc_alcohol,perc_no_previous,percent_speeding,color=cluster)) + geom_point() #with PAM, we see that our 2 clusters overlap and that there isn't really two distinct clusters. #This yields me to believe that the two clusters contain relatively similar data and it would be difficult to establish them as two distinct groupings based on these variables.</p>
<p>pamclust %&gt;% group_by(cluster) %&gt;% summarize_if(is.numeric,mean,na.rm=T) #Here we have the means for each cluster based on each variable. As we can see our cluster means for all three variables are very similar, which is probably what yielded our indistinctive cluster graph.</p>
<p>plot(pam1,which=2) #Here we have our average silhouette width from PAM, which is 0.36. This indicates that our structure is weak and could be artificial. This means our clusters based on these variables are not reliable.</p>
</div>
